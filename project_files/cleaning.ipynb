{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Pre-Processing Airbnb Data\n",
    "<p> Now that we have a good understanding of what our data looks like, the Airbnb datasets provided need to be cleaned and edited for optimal model usage. This includes performing initial feature selection, imputing missing data, examining collinearity, performing variable transformations, and further pre-processing.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNNc\n",
    "import sklearn.metrics as Metrics\n",
    "import warnings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "BNB_BLUE = '#007A87'\n",
    "BNB_RED = '#FF5A5F'\n",
    "BNB_DARK_GRAY = '#565A5C'\n",
    "BNB_LIGHT_GRAY = '#CED1CC'\n",
    "\n",
    "# Global settings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Feature Selection\n",
    "<p>As a baseline, we can start by removing features that we intuitively sense will not be of great importance to a listing's price. These can be further explored later, if need be. This includes 18 features:</p>\n",
    "* `scrape_id`: Not related to actual property data.\n",
    "* `last_scraped`: All within first three days of January, not related to actual property data.\n",
    "* `picture_url`: Could perform visual analysis and mentioned in 'Further Exploration' in the overview. We, however, will not do that. \n",
    "* `host_id`: Ties a host to the property; high unique count makes it unattractive to use.\n",
    "* `host_name`: Not related to actual property data; no textual importance.\n",
    "* `host_since`: Not specific to the listing - arguable how it can be a beneficial feature; no textual importance.\n",
    "* `host_picture_url`: Irrelevant to property data; could perform visual analysis.\n",
    "* `street`: Generic names; location data captured with lower unique count in other geographical features.\n",
    "* `neighbourhood`: The `neighbourhood_cleansed` feature presents the same data in a better format.\n",
    "* `state`: All listings are in the state of NY - this is useless. We clean before removing this feature just in case.\n",
    "* `market`: All listings should be in the NY market - this is useless. \n",
    "* `country`: All listings are in the USA - this is useless.\n",
    "* `weekly_price`: Function of daily price - should not be a predictor.\n",
    "* `monthly_price`: Function of daily price - should not be a predictor.\n",
    "* `calendar_updated`: Does not say much about the property data.\n",
    "* `calendar_last_scraped`: All within first three days of January, irrelevant to actual property data.\n",
    "* `first_review`: Time irrelevant to property data, high unique count.\n",
    "* `last_review`: Time irrelevant to property data, high unique count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the data \n",
    "listings = pd.read_csv('datasets/listings.csv', delimiter=',')\n",
    "\n",
    "# Split into predictor and response\n",
    "y = listings['price']\n",
    "del listings['price']\n",
    "\n",
    "# Store number of entries and features\n",
    "entries = listings.shape[0]\n",
    "features = listings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Features to drop\n",
    "bad_features = ['scrape_id', 'last_scraped', 'picture_url', 'host_picture_url', \n",
    "                'host_id', 'neighbourhood', 'state', 'market', 'country',\n",
    "                'weekly_price', 'monthly_price', 'calendar_last_scraped',\n",
    "                'host_name', 'host_since', 'street', 'calendar_updated',\n",
    "                'first_review', 'last_review']\n",
    "\n",
    "listings.drop(bad_features, axis=1, inplace=True)\n",
    "features = listings.shape[1]\n",
    "\n",
    "print 'Number of entries:', entries\n",
    "print 'Number of features:', features\n",
    "listings.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Emptiness and Initial Cleaning\n",
    "\n",
    "<p>Before imputing missing values, we should examine the percentage of values that are missing from each feature. Imputing data for a feature with too much missing data can bias the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# percent_empty\n",
    "# \n",
    "# Function to return percent of missing data in column\n",
    "# Input: df (data frame)\n",
    "# Output: None\n",
    "def percent_empty(df):\n",
    "    \n",
    "    bools = df.isnull().tolist()\n",
    "    percent_empty = float(bools.count(True)) / float(len(bools))\n",
    "    \n",
    "    return percent_empty, float(bools.count(True))\n",
    "\n",
    "# Store emptiness for all features\n",
    "emptiness = []\n",
    "\n",
    "missing_columns = []\n",
    "\n",
    "# Get emptiness for all features\n",
    "for i in range(0, listings.shape[1]):\n",
    "    p, n = percent_empty(listings.iloc[:,i])\n",
    "    if n > 0:\n",
    "        missing_columns.append(listings.columns.values[i])\n",
    "    emptiness.append(round((p), 2))\n",
    "    \n",
    "empty_dict = dict(zip(listings.columns.values.tolist(), emptiness))\n",
    "\n",
    "# Plot emptiness graph\n",
    "empty = pd.DataFrame.from_dict(empty_dict, orient = 'index').sort_values(by=0)\n",
    "ax = empty.plot(kind = 'bar', color='#E35A5C', figsize = (16, 5))\n",
    "ax.set_xlabel('Predictor')\n",
    "ax.set_ylabel('Percent Empty / NaN')\n",
    "ax.set_title('Feature Emptiness')\n",
    "ax.legend_.remove()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percent emptiness graph shows that `square_feet` is over 90% empty. This is too empty for imputation, so we remove this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listings.drop('square_feet', axis=1, inplace=True)\n",
    "missing_columns.remove('square_feet')\n",
    "features = listings.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We need to also make sure making sure that all quantitative predictors and response variables are float. This will allow us to better deal with categorical data, and NaN entries in the float data.</p>\n",
    "\n",
    "#### Erroneous Entries\n",
    "We also remove entries (listings) that have faulty data such as:\n",
    "\n",
    "* There are 0 bedrooms\n",
    "* There are 0 bathrooms\n",
    "* There are 0 beds\n",
    "* Price is \\$0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to floats\n",
    "y = y.apply(lambda s: float(s[1:].replace(',','')))\n",
    "listings['extra_people'] = listings['extra_people'].apply(lambda s: float(s[1:].replace(',','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of columns to be converted to floating point\n",
    "to_float = ['id', 'latitude', 'longitude', 'accommodates',\n",
    "            'bathrooms', 'bedrooms', 'beds', 'guests_included', \n",
    "            'extra_people', 'minimum_nights', 'maximum_nights', \n",
    "            'availability_30', 'availability_60', 'availability_90', \n",
    "            'availability_365', 'number_of_reviews', 'review_scores_rating', \n",
    "            'review_scores_accuracy', 'review_scores_cleanliness', \n",
    "            'review_scores_checkin', 'review_scores_communication', \n",
    "            'review_scores_location', 'review_scores_value', 'host_listing_count']\n",
    "\n",
    "# Converted columns to floating point\n",
    "for feature_name in to_float:\n",
    "    listings[feature_name] = listings[feature_name].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete bad entries\n",
    "listings = listings[listings.bedrooms != 0]\n",
    "listings = listings[listings.beds != 0]\n",
    "listings = listings[listings.bathrooms != 0]\n",
    "\n",
    "listings = listings.join(y)\n",
    "listings = listings[listings.price != 0]\n",
    "\n",
    "print 'Number of entries removed: ', entries - listings.shape[0]\n",
    "entries = listings.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trimming Neighborhood Entries\n",
    "<p>When we explored our data we saw that geography was very important to pricing, especially on Manhattan. The `neighbourhood_cleansed` feature could therefore be important. Looking at the distribution below we notice it is heavily left-skewed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get number of listings in neighborhoods\n",
    "nb_counts = Counter(listings.neighbourhood_cleansed)\n",
    "tdf = pd.DataFrame.from_dict(nb_counts, orient='index').sort_values(by=0)\n",
    "\n",
    "# Plot number of listings in each neighborhood\n",
    "ax = tdf.plot(kind='bar', figsize = (50,10), color = BNB_BLUE, alpha = 0.85)\n",
    "ax.set_title(\"Neighborhoods by Number of Listings\")\n",
    "ax.set_xlabel(\"Neighborhood\")\n",
    "ax.set_ylabel(\"# of Listings\")\n",
    "plt.show()\n",
    "\n",
    "print \"Number of Neighborhoods:\", len(nb_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We see that the majority of the neighborhoods have less than 100 listings. We currently have 186 neighborhoods - all of these categorical predictors when one-hot encoded will weaken predictive power, so we will only keep neighborhoods with more than 100 listings. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete neighborhoods with less than 100 entries\n",
    "for i in nb_counts.keys():\n",
    "    if nb_counts[i] < 100:\n",
    "        del nb_counts[i]\n",
    "        listings = listings[listings.neighbourhood_cleansed != i]\n",
    "\n",
    "# Plot new neighborhoods distribution\n",
    "tdf = pd.DataFrame.from_dict(nb_counts, orient='index').sort_values(by=0)\n",
    "ax = tdf.plot(kind='bar', figsize = (22,4), color = BNB_BLUE, alpha = 0.85)\n",
    "ax.set_title(\"Neighborhoods by House # (Top 48)\")\n",
    "ax.set_xlabel(\"Neighborhood\")\n",
    "ax.set_ylabel(\"# of Listings\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print 'Number of entries removed: ', entries - listings.shape[0]\n",
    "entries = listings.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity and More Preprocessing \n",
    "<p>Looking through `zipcode` and `city` we realize that there is a lot of erroneous and incomplete data in these features. We will likely remove these features, but will examine multicollinearity to see how much is captured by other geographical features comparably. Before doing so, we need to deal with our missing values. To examine multicollinearity, we will temporarily drop all NaN values and label encode our data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode_categorical\n",
    "# \n",
    "# Function to label encode categorical variables.\n",
    "# Input: array (array of values)\n",
    "# Output: array (array of encoded values)\n",
    "def encode_categorical(array):\n",
    "    if not array.dtype == np.dtype('float64'):\n",
    "        return preprocessing.LabelEncoder().fit_transform(array) \n",
    "    else:\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Temporary listings dataframe\n",
    "temp_listings = listings.copy()\n",
    "\n",
    "# Delete additional entries with NaN values\n",
    "temp_listings = temp_listings.dropna(axis=0)\n",
    "\n",
    "# Encode categorical data\n",
    "temp_listings = temp_listings.apply(encode_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute matrix of correlation coefficients\n",
    "corr_matrix = np.corrcoef(temp_listings.T)\n",
    "\n",
    "corr_df = pd.DataFrame(data = corr_matrix, columns = temp_listings.columns, \n",
    "             index = temp_listings.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display heat map \n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.pcolor(corr_matrix, cmap='RdBu')\n",
    "plt.xlabel('Predictor Index')\n",
    "plt.ylabel('Predictor Index')\n",
    "plt.title('Heatmap of Correlation Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>After inspecting the data, we notice that `zipcode` and `city` are very correlated to each other - and both are also fairly correlated to latitude and longitude. Since these features are very messy, and have a high count of uniques (180+ for each) we will remove them. We also notice that the availability features (`availability_30`, `availability_60`, `availability_90`, and `availability_365`) are all highly correlated. We remove all but `availability_365`, as it is the least correlated to the other features.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unsuppress Output\n",
    "pd.options.display.max_columns = 5\n",
    "pd.options.display.max_rows = 5\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove features\n",
    "listings.drop(['zipcode', 'city', 'availability_30', \n",
    "               'availability_60', 'availability_90'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_columns.remove('zipcode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Imputation on Missing Values\n",
    "<p> To impute, we will take a slightly novel approach with KNN. We have multiple features with missing values, and KNN cannot impute on a column if other features have blank (NaN) entries. We can disregard features with missing values, but we could lose predictive power doing so. For example, the `beds` feature has less than 1% of it's data missing - if we are imputing on another feature (ie. `bedrooms`) and don't use `beds`, we sacrifice the predictive power it could offer.</p>\n",
    "\n",
    "<p>The approach we took is as follows: go through all features with missing values, and cross validate to find the best *k* for each feature. Since there is numerous missing columns, we need to temporarily impute using the median for all quantitative variables (there are no categorical variables with missing features in our model at this point), except the target feature, to be able to conduct KNN on a given column. We repeat this method for all columns with missing entries to perform KNN across all columns.<p/>\n",
    "\n",
    "<p>Since `property_type` is the only categorical variable with missing values left, we will impute it at the end with KNN classification (not regression). Finally, we drop the `name` predictor because it is unique for every property. We later use this predictor in a Bag of Words feature we create, but `name` itself will not be useful predictor for us. We leave `id` in for now as we will use it to build new features later.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove features\n",
    "listings.drop('name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KNN_predict\n",
    "# \n",
    "# Function to predict missing data using KNN\n",
    "# Input: df_missing (dataframe)\n",
    "#        df_filled (dataframe)\n",
    "#        column_name (string)\n",
    "#        k (integer)\n",
    "# Output: df_predict (dataframe of predicted values)\n",
    "def KNN_predict(df_missing, df_temp, df_filled, column_name, k):\n",
    "    \n",
    "    # Training and test set\n",
    "    y = df_filled[column_name]\n",
    "    X_filled = df_filled.drop(column_name, axis = 1)\n",
    "    X_missing = df_temp.drop(column_name, axis = 1)\n",
    "    \n",
    "    # Predict with KNN\n",
    "    if df_filled[column_name].dtype == np.dtype('float64'):\n",
    "        knn = KNN(n_neighbors = k, n_jobs = -1)\n",
    "    else:\n",
    "        knn = KNNc(n_neighbors = k, n_jobs = -1)\n",
    "    knn.fit(X_filled, y)\n",
    "    df_predict = df_missing.copy()\n",
    "    df_predict[column_name] = knn.predict(X_missing)\n",
    "    \n",
    "    return df_predict\n",
    "\n",
    "# KNN_fill\n",
    "# \n",
    "# Function to predict missing data for all columns using KNN\n",
    "# and temporary median displacement for other missing values in\n",
    "# other columns\n",
    "# Input: df (dataframe)\n",
    "#        missing_columns (list of strings)\n",
    "# Output: df_final (filled dataframe)\n",
    "def KNN_fill(df, missing_columns):\n",
    "    \n",
    "    # Separate the rows with missing data information\n",
    "    df_missing = df[pd.isnull(df).any(axis=1)]\n",
    "    df_filled = df[~pd.isnull(df).any(axis=1)]\n",
    "    test_ind = int(0.3 * len(df_filled.index))\n",
    "\n",
    "    # Find an appropriate k for KNN\n",
    "    best_ks = [] \n",
    "\n",
    "    # Go through all columns with missing data, skip property type\n",
    "    for column in missing_columns:\n",
    "        # Impute with median temporarily (only quantitative missing values in our\n",
    "        # model at this point so no need for mode)\n",
    "        temp_df = df_missing.copy()\n",
    "        for c in missing_columns:\n",
    "            # Do not impute selected column\n",
    "            if c != column:\n",
    "                temp_df[c].fillna((temp_df[c].median()), inplace = True)\n",
    "\n",
    "        # Create 10 simulated test and train set from df_filled\n",
    "        for k in range(10):  \n",
    "            RSS = []\n",
    "            scores = []\n",
    "            \n",
    "            # For each simulated test and train set, try k-values: 1 to 15 (counting by 2)\n",
    "            for k in range(1, 15, 2):\n",
    "                df_shuffled = df_filled.sample(frac=1)\n",
    "                df_test = df_shuffled.iloc[:test_ind, :]\n",
    "                df_train = df_shuffled.iloc[test_ind:, :]\n",
    "\n",
    "                #Fill rows with missing information\n",
    "                df_pred = KNN_predict(df_test, df_test, df_train, column, k)\n",
    "\n",
    "                #Compute score of filled in data\n",
    "                if df[column].dtype == np.dtype('float64'):\n",
    "                    RSS.append(((df_pred[column] - df_test[column])**2).mean())\n",
    "                else:\n",
    "                    scores.append(Metrics.accuracy_score(df_test[column], df_pred[column], normalize = True))\n",
    "\n",
    "\n",
    "            # Record the k that yields best score\n",
    "            if df[column].dtype == np.dtype('float64'):\n",
    "                best_ks.append(np.argmin(RSS) + 1)\n",
    "            else:\n",
    "                best_ks.append(np.argmax(scores) + 1)\n",
    "\n",
    "        # Take the mean of the best k over 100 simulations\n",
    "        best_k = int(np.mean(best_ks))\n",
    "\n",
    "        # Fill rows with missing information, using the optimal k\n",
    "        df_missing = KNN_predict(df_missing, temp_df, df_filled, column, best_k)\n",
    "        \n",
    "    # Concatenate rows with no missing info and the row we filled in\n",
    "    df_final = pd.concat([df_filled, df_missing])\n",
    "    df_final = df_final.sort_index()\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "listings = listings.apply(encode_categorical)\n",
    "\n",
    "# Impute missing values with KNN\n",
    "del listings['price']\n",
    "listings_clean = KNN_fill(listings, missing_columns)\n",
    "\n",
    "# Append price to data\n",
    "listings = listings.join(y)\n",
    "listings_clean = listings_clean.join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Round values\n",
    "for c in missing_columns:\n",
    "    listings_clean[c] = listings_clean[c].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output cleaned dataframe\n",
    "listings_clean.to_csv(path_or_buf='datasets/listings_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
