{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we seek to explore additional features outside of those that were originally given to us in the *listings.csv* file. Specifically in this section we will explore:\n",
    "\n",
    "1. **Proximity to Central Park:** Properties that line the outside of Central Park often carry higher prices as a result of the scenic views they have to offer - which is often a feature in high demand especially amongst tourists.\n",
    "\n",
    "2. **Sentiment Analysis on Listing Title and Reviews:** This helps us quantify our reviews. We are able to see how positive and negative reviews affect listing price points. We know that a strong positive reviews can stimulate demand, while weak and negatives add a downward pressure on price. In addition, we know that more attractive titles tend to get more clicks and page views than those with bland or even negative titles. We run sentiment analysis on both the listing title and the reviews to help quantify these phenomena.\n",
    "\n",
    "3. **Distance to Neaerest Subway:** Transportation in New York City is largely dependent on the New York City subway. Therefore there exists a premium on listings that are near the subway and offer reduced travel time.\n",
    "\n",
    "4. **Seasonality Data:** Understanding the effect that specific dates, weeks, and seasons have on listing prices in New York City.\n",
    "\n",
    "These additional features hope to serve as additional data points that will help us better create a model that accurately outputs the predicted price for which a listing should be priced at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from matplotlib.path import Path\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GLOBAL VARIABLES\n",
    "BNB_BLUE = '#007A87'\n",
    "BNB_RED = '#FF5A5F'\n",
    "BNB_DARK_GRAY = '#565A5C'\n",
    "BNB_LIGHT_GRAY = '#CED1CC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listings = pd.read_csv('listings_clean.csv', delimiter=',').iloc[:, 1:]\n",
    "calendar = pd.read_csv('calendar.csv', delimiter=',', usecols=range(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Proximity to Central Park:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in encoding a feature aimed at identifying properties on the perimeter of Central Park is to identify potential idiosyncratic points of error. Central Park offers scenic views and a central location that often catches the attention of a potential visitors looking for an Airbnb. In addition, Central Park is one of the top tourist spots people visit when coming to New York City. By identifying these properties, we hope to create a feature that better enables us to predict price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "streets = listings['street'].tolist()\n",
    "streets_cleansed = []\n",
    "\n",
    "# Clean Street Data\n",
    "for street in streets:\n",
    "    i = street.find(',')\n",
    "    streets_cleansed.append(street[:i])\n",
    "\n",
    "listings['streets_cleansed'] = streets_cleansed\n",
    "\n",
    "# List of Streets on Central Park\n",
    "park_streets = ['West 110th Street', 'W 110th St', '5th Ave', '5th Avenue', 'Central Park', \n",
    "                'Central Park South', 'Central Park West', 'Central Park North', 'Central Park East', 'West 59th Street']\n",
    "\n",
    "listings['on_park'] = listings['streets_cleansed'].isin(park_streets)\n",
    "\n",
    "park = listings[listings['on_park'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method shown above takse preselected streets otulining the park and compares it with the street names in the listings dataframe to find which listings border Central Park. The problem with this method is that is relatively impercecise as far as determining actual proximity and location of these listings. It also produced a relatively small number of homes that bordered Central Park. This shows us that the addresses provided were fairly inaccurate and we ended up removing the *street* column all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verts = [\n",
    "    (-73.958546, 40.801266),\n",
    "    (-73.948807, 40.797251),\n",
    "    (-73.972904, 40.764011),\n",
    "    (-73.982735, 40.767682),\n",
    "    (0., 0.), # ignored\n",
    "        ]\n",
    "\n",
    "# draws rectangle outlining Central Park\n",
    "codes = [Path.MOVETO,\n",
    "         Path.LINETO,\n",
    "         Path.LINETO,\n",
    "         Path.LINETO,\n",
    "         Path.CLOSEPOLY,\n",
    "         ]\n",
    "\n",
    "path = Path(verts, codes)\n",
    "patch = patches.PathPatch(path,facecolor = 'none', lw=2)\n",
    "\n",
    "on_park = []\n",
    "for index, row in listings.iterrows():\n",
    "    on_park.append(path.contains_point((row['longitude'], row['latitude']) , radius = -.005))\n",
    "\n",
    "    \n",
    "listings['on_park'] = on_park\n",
    "park = listings[listings['on_park'] == True]\n",
    "\n",
    "# Remove non-exact entries\n",
    "park = park[park['is_location_exact'] == 1]\n",
    "\n",
    "print park.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listings['on_park'].loc[listings['on_park'] == False] = 0.0\n",
    "listings['on_park'].loc[listings['on_park'] == True] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "on_park_data = listings['on_park']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this, we first draw a rectangle based on the latitude and longitude coordinates outlining Central Park. From this using a radius parameter we find the listings that fall within this radius parameteter and stores the boolean value of whether a point falls on/withing the park and stores the value in the *on_park* rray. From there we creates a column on our *listings* data set tracking the boolean values.\n",
    "\n",
    "The scatter plot below grpahically highlights the listings that are classified as being on Central Park. The number of lisitngs that are identified as being on Central Park is dependent on how we set our radius in our *path.contains_point* function. As expected, as radius grows as does the number of listings. However, it is important to tune this radius parameter to achieve a setting that maximizes predictive potency. \n",
    "\n",
    "We can now create an additioanl features that tracks whether a listing lies on Central Park. If it lies on Central Park, will will denote the column *on_park* with a 0. However, if the listing falls in the acceptable range then will assign a binary 1, indicating that it falls 'on' central park."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Outline price buckets\n",
    "intervals = [0,100,300,10000]\n",
    "leg_labels = []\n",
    "\n",
    "# Get Labels for Legend\n",
    "for i in range(0,len(intervals) - 1):\n",
    "    if i == len(intervals) - 2:\n",
    "        leg_labels.append('\\${}+'.format(intervals[i]))\n",
    "    else:\n",
    "        leg_labels.append(\"\\${}-\\${}\".format(intervals[i], intervals[i+1]))    \n",
    "\n",
    "buckets = []\n",
    "x = listings.drop('id', 1)\n",
    "\n",
    "# Divide up into buckets\n",
    "for i in range(0, len(intervals) - 1):\n",
    "    buckets.append(x[(x['price'] > intervals[i]) & (x['price'] < intervals[i+1])])\n",
    "\n",
    "# Plot listings on scatter\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "# Plot all listings by price bucket\n",
    "for i in range(0, len(buckets)):\n",
    "    ax.scatter(buckets[i]['longitude'], buckets[i]['latitude'], alpha = alphas[i], c=colors[i], s=2)\n",
    "\n",
    "# Plot Central Park Listings\n",
    "ax.scatter(park['longitude'], park['latitude'], s=20, alpha = 1, c = BNB_RED)\n",
    "\n",
    "# Figure attributes\n",
    "ax.set_title('New York City - AirBnB Listings')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.legend(labels=leg_labels, loc = 'best')\n",
    "ax.add_patch(patch)\n",
    "ax.set_xlim(-74,-73.9)\n",
    "ax.set_ylim(40.75,40.85)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentiment Analysis on Listing Title and Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Building a Bag-of-Words Feature\n",
    "\n",
    "Using a little bit of Natural Language Processing, we set out to create a bag-of-words feature. To do this, we disregard grammar and average the polarity of a given segment of text to arrive at a sentiment analysis feature which is represented in this case as a float. \n",
    "\n",
    "#### Concatenation of Listing Titles with Reviews\n",
    "We want to perform sentiment analysis on a 'segment of text', and our only very meaningful textual features are the listing title and any reviews associated with that listing. We first concatenate the two together (or maybe more if there's more than one review).\n",
    "\n",
    "###### Load the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the data \n",
    "listings = pd.read_csv('listings_clean.csv', delimiter=',')\n",
    "listing_uncleaned = pd.read_csv('listings.csv', delimiter = ',')\n",
    "reviews = pd.read_csv('reviews.csv', delimiter=',')\n",
    "\n",
    "# Split into predictor and response\n",
    "y = listings['price']\n",
    "\n",
    "# Append price at the end of the listings table\n",
    "del listings['price']\n",
    "x = listings\n",
    "              \n",
    "listings = listings.join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = []\n",
    "for item in listings['id'].values:\n",
    "    titles.append(listing_uncleaned['name'].loc[listing_uncleaned['id'] == item].values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x['name'] = titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a preview of some of the titles in our listings datafile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x['name'].head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, below are some of the reviews on the listings that guests have left behind. Right away we can see that the reviews range with regards to their lengths and content. In addition, they also range in the language in which they are written in. For exampel the second review appears to be written in German, while the fifth review appears to be written in French. This is something we should be cognizant of when constructing our bag of words feature.\n",
    "\n",
    "We may have to do a little bit of cleaning on our review dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preview Dataframe\n",
    "reviews.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of our reviews aren't in English, that there are additional non-English characters and symbols (\\n) and also we know that some cells are empty. We can manually try to clean this data and also use NLP libraries to help out. We do a bit of both and we see the libraries ```nltk``` and ```text blob```.\n",
    "\n",
    "##### Drop Rows with NaN values (empty reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Reviews before NaNs dropped: ', reviews.shape[0]\n",
    "# Delete reviews entries with NaN values (empty review)\n",
    "reviews = reviews.dropna(axis=0)\n",
    "print 'Reviews after NaNs dropped: ', reviews.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatentation Function:**\n",
    "We need to write function to combine our two datasets so that every unique listing id not only links to its property features, but also the reviews associated with that property. These are stored in ```reviews.csv``` while the listing titles are stored in ```listings.csv```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates empty array to store concatenated text\n",
    "empty_concat = []\n",
    "\n",
    "# iterates through each listing id\n",
    "for item in x['id'].unique():\n",
    "    # checks to see if the corresponding listing id has a review\n",
    "    # if so it concatenates the listing title with the reivew\n",
    "    if item in reviews['listing_id'].unique():\n",
    "        empty_concat.append(str(x['name'].loc[x['id'] == item].values[-1]) + ' ' + str(reviews['comments'].loc[reviews['listing_id'] == item].values[:-1]))\n",
    "    else:\n",
    "        empty_concat.append(str(x['name'].loc[x['id'] == item].values[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now stored our concatenated reviews and listings and need to think about cleaning up our \"bag of words\" a little bit. If we remove all tokens that have to do with NYC neighborhoods, this can help reduce collinearity with our other geographic-based features. Lets get those terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighborhood_words = []\n",
    "\n",
    "for item in listings['id'].values:\n",
    "    neighborhood_words.append(listing_uncleaned['neighbourhood_cleansed'].loc[listing_uncleaned['id'] == item].values[-1])\n",
    "\n",
    "x['neighborhood_names'] = neighborhood_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Neighborhood words to remove geographic correlation from our 'bag-of-wods'\n",
    "nbs = x['neighborhood_names'].unique().tolist()\n",
    "nbs = [nb.lower().split() for nb in nbs]\n",
    "nbs = set([item for sublist in nbs for item in sublist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the reviews in shape for analysis\n",
    "\n",
    "An important tool we used was stemming: grabbing the \"stem\" of a word which is the same for all conjugations, declensions, forms etc. of the word. This is great for quickly identifying connotation or polarity and also counting frequency of word appearances. If a word is neither an English stop-word or related to a NYC neighborhood, we add it to our 'bag of words' for a given listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "bags = empty_concat\n",
    "bag_of_words = []\n",
    "\n",
    "# Make characters all english alphabet\n",
    "for review in bags:\n",
    "    # Use regular expressions to do a find-and-replace for alphabet\n",
    "    replaced = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
    "        \n",
    "    # All lower\n",
    "    lower_case = replaced.lower()\n",
    "    \n",
    "    # Put in list\n",
    "    words = lower_case.split()\n",
    "    \n",
    "    # Exclude stop-words and neighborhood words\n",
    "    words = [stemmer.stem(w) for w in words if ((not w in stops) & (not w in nbs))]\n",
    "    listing_words = ' '.join(words)     \n",
    "    bag_of_words.append(listing_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save to bag_of_words.csv:\n",
    "Because this was such a long operation, it's good to save time and store the data in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Output bag_of_words feature to csv\n",
    "bag = pd.DataFrame(x['id'])\n",
    "bag['bag_of_words'] = bag_of_words\n",
    "bag.to_csv('bag_of_words.csv')\n",
    "\n",
    "bag.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Sentiment Prediction Method\n",
    "\n",
    "Once we have our bags of words, we need to perform sentiment prediciton. We implemented a manual strategy and compared it to NLP package libraries. First with the manual strategy which compared the stems in a bag to a publicly available corpus of positive and negative words available at:\n",
    "\n",
    "http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load positive and negative word corpera\n",
    "positive = pd.read_csv('positive-words.txt', encoding='latin1')\n",
    "negative = pd.read_csv('negative-words.txt', encoding='latin1')\n",
    "\n",
    "pos_lib_full = positive.iloc[:, 0].tolist()\n",
    "neg_lib_full = negative.iloc[:, 0].tolist()\n",
    "new = []\n",
    "\n",
    "for each in positive.iloc[:, 0].tolist():\n",
    "    word = each.encode('ascii', 'ignore')\n",
    "    neg_lib_full.append(word)\n",
    "\n",
    "for each in negative.iloc[:, 0].tolist():\n",
    "    word = each.encode('ascii', 'ignore')\n",
    "    new.append(word)\n",
    "\n",
    "# Make corpus sets for faster lookup\n",
    "pos_lib_stems = [stemmer.stem(str(w)) for w in pos_lib_full]\n",
    "pos_lib = set(pos_lib_full + pos_lib_stems)\n",
    "\n",
    "neg_lib_full = new\n",
    "neg_lib_stems = [stemmer.stem(str(w)) for w in neg_lib_full]\n",
    "neg_lib = set(neg_lib_full + neg_lib_stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our corpera are loaded, we make a function corpus_predict() which outputs polarity averages for a list of segments of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Score Positivity/Negativity from Reviews\n",
    "\n",
    "# Count positive and negative word appearances,\n",
    "# returns polarity averages as a list for each review\n",
    "def corpus_predict(texts):\n",
    "    polarities = []\n",
    "    \n",
    "    for i in range(0, len(texts)):\n",
    "        opinion = texts[i].split()\n",
    "        pos_count, neg_count = 0., 0.\n",
    "        polarity = 0.5\n",
    "        for word in opinion:\n",
    "            if word in pos_lib:\n",
    "                pos_count += 1.\n",
    "            elif word in neg_lib:\n",
    "                neg_count += 1.\n",
    "        \n",
    "        if (pos_count == 0.) & (neg_count == 0.):\n",
    "            pass\n",
    "        else:\n",
    "            polarity = pos_count/(pos_count + neg_count)\n",
    "        polarities.append(polarity)\n",
    "        \n",
    "    return polarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_bag = bag['bag_of_words'].tolist()\n",
    "man_polarities = corpus_predict(full_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Sentiment Prediction Methods\n",
    "\n",
    "We also tried sentiment prediction on the same bags of words with Natural Language Tool Kit and TextBlob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "nltk_polarities = []\n",
    "\n",
    "for each in full_bag:\n",
    "    each = \" \".join(each)\n",
    "    ss = sid.polarity_scores(each)\n",
    "    nltk_polarities.append(ss.values())\n",
    "print 'Polarities appended in form [neg, neu, pos, compound]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "tb_polarities = []\n",
    "\n",
    "for i, each in enumerate(full_bag):\n",
    "    blob = TextBlob(each)\n",
    "    tb_polarities.append(blob.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initiate Figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 6))\n",
    "BINS = 30\n",
    "\n",
    "# Plot Manual Polarity Average Distribution\n",
    "ax1.hist(man_polarities, bins = BINS, color = BNB_RED)\n",
    "ax1.set_title('Distribution of Manual Polarity Averages')\n",
    "ax1.set_xlim([0,1])\n",
    "ax1.set_ylim([0,9000])\n",
    "\n",
    "# Plot Text Blob Polarity Average Distribution\n",
    "ax2.hist(tb_polarities, bins = BINS, color = BNB_BLUE)\n",
    "ax2.set_title('Distribution of TextBlob Polarity Averages')\n",
    "ax2.set_xlim([0,1])\n",
    "ax2.set_ylim([0,9000])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can compare the the manual sentiment analysis versus the libary package method from Text Blob. Our first version had a tendency to classify text segments as overly positive, probably because our negative words corpus wasn't exhaustive enough. This could be fixed, but we decided to go with the Text Blob method for polarity averages as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Most Common Words:\n",
    "\n",
    "Additionally its useful to know the most common words in our listing reviews as this can be used to create multinomial feature vectors for each listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flatten List\n",
    "list_bag = [w.split() for w in full_bag]\n",
    "all_words = [item for sublist in list_bag for item in sublist]\n",
    "\n",
    "# Get 1000 most common tokens\n",
    "most_common = Counter(all_words).most_common(1000)\n",
    "\n",
    "# Remove words shorter than length 2\n",
    "common_words = [w for w in most_common if len(w[0]) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plt.figure(111, figsize = (10,4))\n",
    "\n",
    "plt.bar(range(len(common_words)), [w[1] for w in common_words], align='center', color = BNB_BLUE)\n",
    "plt.xlim([0,1000])\n",
    "plt.title('Most Common Tokens Distribution')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This right-skewed curve is what one could expect for the most appearing tokens - it also lends itself well to creating a multinomial feature vector where we can count how many times each bag-of-words has any of the most-appearing tokens. We'll leave that for another day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Distance to Neaerest Subway\n",
    "\n",
    "There is a premium associated with living near a subway station in New York City. The reason being is that living near a subway station makes travel in the city much easier, and opens up the possibility for you to to travel virtually anywhere in the city by train. Just as permanent residents in New York City staying in apartments pay a premium for living next to a subway stop, so must users looking to rent an Airbnb. Therefore, by identifying the distance from that each listing is from its nearest subway station we are better able to account for this location price premium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# taken from https://data.ny.gov/Transportation/NYC-Transit-Subway-Entrance-And-Exit-Data/i9wp-a4ja/data\n",
    "transit = pd.read_csv('Transit.csv', delimiter = ',')\n",
    "\n",
    "# preview of transit data\n",
    "transit.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'The shape of our transit data is ' + str(transit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'There are ' + str(len(transit['Station Name'].unique())) + ' unique stations in our transit data set.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates empty array to story longitude and latitude values of station entrances\n",
    "avg_lat = np.zeros([len(transit['Station Name'].unique()),])\n",
    "avg_long = np.zeros([len(transit['Station Name'].unique()),])\n",
    "\n",
    "# iterates through each station and averages longitude and latitude coordinates\n",
    "counter = 0\n",
    "for item in transit['Station Name'].unique():\n",
    "        avg_lat[counter] = np.mean(transit['Entrance Latitude'].loc[transit['Station Name'] == item])\n",
    "        avg_long[counter] = np.mean(transit['Entrance Longitude'].loc[transit['Station Name'] == item])\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are several entrance for each subway stop in New York City, we chose to simplify our calculation of distance from the subway by averaging the latitude and longitude for the subway entrances for each subway stop. This will help expedite the run time of our function given that there are 1,868 total subway stops in New York, which when we iterate through each listing would have a high expected run time, making it computationally expensive. So by averaging, we are able to expedite the computational calculation time, without sacrificing degrees of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates a pandas dataframe to store the average longitude and latitudes to one station location\n",
    "transit_location = pd.DataFrame({'Station Name': transit['Station Name'].unique(),'Latitude': avg_lat, 'Longitude': avg_long})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creates an empty array to store the distance from the nearest subway for each listing\n",
    "dist_transit = np.zeros([len(listings),])\n",
    "\n",
    "index = 0\n",
    "# iterates through each listing\n",
    "for index, item in listings.iterrows():\n",
    "    min_dist = 1000\n",
    "    \n",
    "    # checks each of the 356 stations for the closest station\n",
    "    for counter, row in transit_location.iterrows():\n",
    "        # measures distance on a Euclidean distance basis\n",
    "        dist = np.sqrt((item['latitude'] - row['Latitude'])**2+(item['longitude'] - row['Longitude'])**2)\n",
    "        # if the current distance is less than the stored minimum distance, than the current distance becomes the\n",
    "        # min distance\n",
    "        if dist < min_dist:\n",
    "            dist_transit[index] = dist\n",
    "            min_dist = dist\n",
    "    index +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each listing in our *listings.csv* file, we go through each subway stop and calculate the distance from the Airbnb listing and the subway stop. For iteration through our listing ID, we store the minimum distance value in a *dist_transit* array, so that we can keep track of the minimum distance to the subway for each listing ID. Because this exercise is computationally expensive, we choose to export our data table to *.csv* file. This way, we can just import the data as an additionally feature and add it to the dataframe from *listings.csv*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Seasonality Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe the effect of seasonality on listing price, we should first visualize how price within price bukcets varies as a funciton of date. To do so, we split our calendar into the three price buckets of \\$0-\\$100, \\$100-\\$300, and \\$300+ that we used define the points on our price scatter plot overlayed against the map of New York City. Using these price buckets, we hope to see how the price of listing is affected based on the date listing is on. In addition, this will help us observe some of the more macro trends regarding the effects of seasonality, holidays, and weekends have on listing price.\n",
    "\n",
    "***NOTE ON CLEANING CALENDAR.CSV:*** Please note that we had to do some manual data cleaning due to the semi-corrupted nature of our data file. The price column in our calendar.csv file was using the comma - ',' - in the csv file to separate prices that were greater than one thousand into two numbers - values before the comma - and - valeus after the commas. So we had to manually go into the csv file and correct these errors, allowing us to conduct our seasonality data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creates empty array to store the listing IDs of into price buckets\n",
    "high_bucket = []\n",
    "mid_bucket = []\n",
    "low_bucket = []\n",
    "\n",
    "# iterates through each row in the calendar dataframe\n",
    "for index, item in calendar.iterrows():\n",
    "    # checks to see which price bucket our listing IDs fall in \n",
    "    if item['listing_id'] in buckets[0]['id'].unique():\n",
    "        low_bucket.append(item)\n",
    "    elif item['listing_id'] in buckets[1]['id'].unique():\n",
    "        mid_bucket.append(item)\n",
    "    elif item['listing_id'] in buckets[2]['id'].unique():\n",
    "        high_bucket.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first sort the listing IDs into arrays based on their corresponding listing prices. The listings IDs that fall in the \\$0-\\$100 price range will have their IDs placed in the *low_bucket*, those listings in the \\$100-\\$300 price range will have their IDs placed in the *mid_bucket*, and those listings in the \\$300+ price range will have their listing IDs placed in the *high_bucket* array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# crtes a calendar dates array consisting of all the unique calendar dates (366)\n",
    "calendar_dates = np.array(calendar['date'].unique())\n",
    "\n",
    "# converts arrays to pandas dataframes\n",
    "high_bucket = pd.DataFrame(high_bucket)\n",
    "mid_bucket = pd.DataFrame(mid_bucket)\n",
    "low_bucket = pd.DataFrame(low_bucket)\n",
    "\n",
    "# empty array to store the average daily price information based on price bucket\n",
    "high_seasonality_data = np.empty([len(calendar_dates),1])\n",
    "mid_seasonality_data = np.empty([len(calendar_dates),1])\n",
    "low_seasonality_data = np.empty([len(calendar_dates),1])\n",
    "\n",
    "index = 0\n",
    "\n",
    "# iterates each day of the calendar\n",
    "for item in calendar_dates:\n",
    "    calendar_dates[index] = datetime.strptime(item,'%m/%d/%y').date()\n",
    "    high_seasonality_data[index, 0] = np.mean(high_bucket['price'].loc[high_bucket['date'] == item])\n",
    "    mid_seasonality_data[index, 0] = np.mean(mid_bucket['price'].loc[mid_bucket['date'] == item])\n",
    "    low_seasonality_data[index, 0] = np.mean(low_bucket['price'].loc[low_bucket['date'] == item])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we aim to do is take the average daily listing price per day for each price bucket and graph it as a function of time over the year. This will help us track how average price within each bucket varies as a function of time. This will enable to view things such as the impact of seasonality and how weekends affect Airbnb listing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# prints all buckets (high, mid, and low) containing seasonality data\n",
    "plt.plot(calendar_dates, high_seasonality_data, label = 'High Price Bucket', color = BNB_RED)\n",
    "plt.plot(calendar_dates, mid_seasonality_data, label = 'Middle Price Bucket', color = BNB_BLUE)\n",
    "plt.plot(calendar_dates, low_seasonality_data, label = 'Low Price Bucket', color = BNB_DARK_GRAY)\n",
    "\n",
    "plt.title('Average Daily Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph we can see small short-term fluctionations in price, seeming to indicate the presense of weekly factor, potentially the categorization of a weekday versus a weekend, causes price to fluctuate. Howver, to verify our findings we must bucket on its own appropriate price scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High Price Bucket:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plots only listings in the high price bucket\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(calendar_dates, high_seasonality_data, color = BNB_RED, label = 'Average Daily Price')\n",
    "plt.plot(calendar_dates, np.median(high_seasonality_data)*np.ones([len(high_seasonality_data),1]), color = BNB_BLUE, label = 'Median Daily Price')\n",
    "\n",
    "plt.title('High Price Bucket')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.legend(loc = 'best')\n",
    "plt.ylim(460,580)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data appears to fluctuate around a median of \\$530 in the high bucket daily price graph. The graph shows an amplitude of about \\$30 in which prices range between around \\$500 and \\$560. The maximum daily price peaks at arond \\$565 on New Years Day, and quickly drops below the its median value. In addition, we can observe the short-term fluctuations seeming to suggest that certain weekly spikes. In fact, if we count the number of spikes we will find that there are indeed 52 weeks, representing the 52 weeks in a year. As far as larger time period trends are concerns, we can see that as we move from March into the spring and summer months, we can observe a price increase that carries us above the median price. However, the price begins to drop as we transition towards the autumn months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Middle Price Bucket:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plots only listings in the middle price bucket\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(calendar_dates, mid_seasonality_data, color = BNB_RED ,label = 'Average Daily Price')\n",
    "plt.plot(calendar_dates, np.median(mid_seasonality_data)*np.ones([len(mid_seasonality_data),1]), color = BNB_BLUE, label = 'Median Daily Price')\n",
    "\n",
    "plt.title('Middle Price Bucket')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The middle price bucket, chart follows a trend similar to that of the high price bucket. This time we see out data centered around a median nightly price of just under \\$180, with an amplitude of about \\$10 causing the prices to range from about \\$170 to \\$190. The trends are largely the same especially when evaluating movements as percentage movements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Low Price Bucket:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plots only listings in the low price bucket\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(calendar_dates, low_seasonality_data, color = BNB_RED ,label = 'Average Daily Price')\n",
    "plt.plot(calendar_dates, np.median(low_seasonality_data)*np.ones([len(low_seasonality_data),1]), color = BNB_BLUE, label = 'Median Daily Price')\n",
    "\n",
    "plt.title('Low Price Bucket - Average Nightly Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.legend(loc = 'best')\n",
    "plt.ylim(70,90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low price bucket data is centered around a median daily price of \\$75, with values mainly ranging from \\$73 to about \\$78. We again see the same weekly spikes in addition to larger seasonal trends in which the transitioning from spring to summer yields higher average daily prices.\n",
    "\n",
    "**Seasonality Take Aways:** We find that cross listings price ranges, that calendar date is an important feature in predicting the exact price of Airbnb listings. This can cause prices to fluctuate around their median based on changing demand that results from changes in the date that someone wishes to stay in a listing. This allows us to draw three key conclusions on the effect of calendar data.\n",
    "\n",
    "* **Weekends:** The weekly spikes in housing prices lead us to believe that there is indeed a strong weekend effect that causes daily prices of Airbnb listings to spike. This intuitively makes sense because we expect people to travel more on weekends as opposed to weekdays. This increased travel on weekends creates an increase in demand that puts an upward pressure on price. As demand drops, following Sunday, so does price. This weekly cycle causes the small fluctuations in the price that we observe on all 4 graphs.\n",
    "\n",
    "\n",
    "\n",
    "* **Spring and Summer Months:** In all price bucket graphs, we can see that April begins to mark the pivot point in which we begin to see a rise in listing prices. The warmer weather, coupled with the fact that children and teenagers finish school lead to an increase in demand for housing in the summer months. This fact is best evidence by the fact that the average daily listing price is nearly always above its median value. As noted before, potential reasons include increased travel in the warmer months. Another potential reason, one in which I've observed first hand, is the fact that there is an increased demand in housing that results from the the influx of summer interns that enter the city. However, given our current data set we cannot conclusively determine the root of this trend. We would need additional data to be able to conclusively determine the cause of this spike.\n",
    "\n",
    "\n",
    "* **Holidays:** One of the more clear examples of holidays affecting pricing data is observed on New Years Day. New Years Eve into New Years Day represents the biggest demand for housing in New York City. Many visitors from around the world travel to New York to watch the world famous ball drop happen in Time Square. This excess demand causes the sharp spike in prices that we observe on 01/01/15. For all three price bucket graphs, the maximum daily price occurs on January 1st, 2015. There is a sharp price drop that occurs following New Years Day as people post-New Years no longer require housing. This is the most clear example of the influence that holiday season can have on housing data because this instance is free from the effects of seasonal impact (spring and summer months).\n",
    "\n",
    "\n",
    "**Using Seasonality to Predict Price:** Our approach to predicting price as a function of seasonality is one that stems from the idea that seasonality is a factor that disturbs natural price equilibrium. This assumption seems reasonable especially from a supply and demand perspective in which we know that the price spikes we observe result from disequilibrium (due to excess demand) that places an upward pressure on price. Therefore, our approach entails using the seasonality charts we produced by price bucket to find calculate the average price disruption that results from listing on certain calendar dates. So for a given date for a given listing, we will take the price that our model outputs and add or subtract the different that exists on the respective price bucket graph the difference between the average daily price and the median price at that date. Such a method will help us capture the trends that we observed above and fairly accuractly predict price as a function of the date.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Feature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# exports transit location / distance information to a csv\n",
    "exported_data = pd.DataFrame({'id': listings['id'].values, 'distance_to_subway': dist_transit, 'on_park': on_park_data.values, 'polarities': tb_polarities})\n",
    "exported_data = exported_data[['id', 'on_park', 'polarities', 'distance_to_subway']]\n",
    "\n",
    "exported_data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exported_data.to_csv('feature_data.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
