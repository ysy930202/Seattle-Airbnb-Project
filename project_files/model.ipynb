{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Tuning\n",
    "<p>Now that we have cleaned our data, we can examine different models and pick the most suitable one. We compare a few baseline models to more advanced ensemble methods. The ensemble methods yield far better results, especially after proper tuning (using GridSearchCV).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import exp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression as LinReg\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "BNB_BLUE = '#007A87'\n",
    "BNB_RED = '#FF5A5F'\n",
    "BNB_DARK_GRAY = '#565A5C'\n",
    "BNB_LIGHT_GRAY = '#CED1CC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models\n",
    "The three baseline models are Linear Regression, Ridge Regression, and Lasso Regression. Before proceeding with regression, we must one-hot encode our categorical variables. One-hot encoding only increases our feature count from 26 to 92 - this is not a lot. Our intensive cleaning process eliminated unimportant categorical variables with high unique-counts, and trimmed the more important ones like `neighbourhoods_cleansed`. We score each baseline model on both R^2 and MAE (median absolute error) to evaluate them, and average the scores from 5-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the data \n",
    "listings = pd.read_csv('listings_clean.csv', delimiter=',').iloc[:, 1:]\n",
    "\n",
    "# read in additional featurs\n",
    "features = pd.read_csv('feature_data.csv', delimiter=',').iloc[:, 1:]\n",
    "features = features.drop('id',1)\n",
    "\n",
    "X = listings.iloc[:, 0:-1]\n",
    "X = X.drop('id', 1)\n",
    "\n",
    "X = pd.concat([X, features], axis=1, join_axes=[X.index])\n",
    "# converts on_park to a float\n",
    "X['on_park']= X['on_park'].astype(float)\n",
    "\n",
    "# Price log transform\n",
    "y = np.log(listings['price'])\n",
    "\n",
    "print 'Number of samples:', X.shape[0]\n",
    "print 'Number of features:', X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply one hot endcoing\n",
    "categorical = (X.dtypes.values != np.dtype('float64'))\n",
    "\n",
    "categorical_names = []\n",
    "for item in X.columns.values:\n",
    "    if X[item].dtype != np.dtype('float64'):\n",
    "        categorical_names.append(item)\n",
    "\n",
    "# Last value in mask is price\n",
    "X_encoded = pd.get_dummies(X, prefix = categorical_names, columns = categorical_names)\n",
    "\n",
    "# Examine the size of the datasets after one-hot\n",
    "print 'Number of features:', X_encoded.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinary Least Squares (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cross validate on 5 folds\n",
    "cv = KFold(n_splits = 5)\n",
    "linreg = LinReg()\n",
    "\n",
    "# Store r2 and median absolute error\n",
    "MAE =[]\n",
    "R2 = []\n",
    "\n",
    "for train, test in cv.split(X_encoded):\n",
    "    # Fit linreg\n",
    "    linreg.fit(X_encoded.iloc[train], y.iloc[train])\n",
    "    \n",
    "    # Predict and score\n",
    "    Y_predict = linreg.predict(X_encoded.iloc[test])\n",
    "    R2.append(linreg.score(X_encoded.iloc[test], y.iloc[test]))\n",
    "    MAE.append(median_absolute_error(y.iloc[test], Y_predict))\n",
    "\n",
    "# Output and store scores after CV\n",
    "lin_testing_set_score = np.mean(R2)\n",
    "lin_median_abs_error = np.mean(MAE)\n",
    "print 'The R^2 score on our testing data is: ' + str(round(lin_testing_set_score, 3))\n",
    "print 'The median absolute error on our testing data is: ' + str(round(lin_median_abs_error, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tuning paramater values for Ridge\n",
    "lambdas = 10.**np.array([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n",
    "\n",
    "# Perform Ridge regression using expanded set of predictors, \n",
    "# choose best regularization parameter lambda using 5-fold x-validation\n",
    "cv = KFold(n_splits = 5)\n",
    "ridge = RidgeCV(alphas = lambdas, fit_intercept = False, normalize = True, cv = 5)\n",
    "\n",
    "# Store r2 and median absolute error\n",
    "MAE =[]\n",
    "R2 = []\n",
    "\n",
    "# Find best score by cross validating over 5 folds\n",
    "for train, test in cv.split(X_encoded):\n",
    "    # Fit linreg\n",
    "    ridge.fit(X_encoded.iloc[train], y.iloc[train])\n",
    "    \n",
    "    # Predict and score\n",
    "    Y_predict = ridge.predict(X_encoded.iloc[test])\n",
    "    R2.append(ridge.score(X_encoded.iloc[test], y.iloc[test]))\n",
    "    MAE.append(median_absolute_error(y.iloc[test], Y_predict))\n",
    "    \n",
    "# Output and store scores after CV\n",
    "ridge_testing_set_score = np.mean(R2)\n",
    "ridge_median_abs_error = np.mean(MAE)\n",
    "print 'The R^2 score on our testing data is: ' + str(round(ridge_testing_set_score,3))\n",
    "print 'The median absolute error on our testing data is: ' + str(round(ridge_median_abs_error,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform Lasso regression using expanded set of predictors, \n",
    "# choose best regularization parameter lambda using 5-fold x-validation\n",
    "cv = KFold(n_splits = 5)\n",
    "lasso = LassoCV(alphas = lambdas, tol = 0.01, fit_intercept = False, normalize = True, cv = 5)\n",
    "\n",
    "# Store r2 and median absolute error\n",
    "MAE =[]\n",
    "R2 = []\n",
    "\n",
    "# Find best score by cross validating over 5 folds\n",
    "for train, test in cv.split(X_encoded):\n",
    "    # Fit linreg\n",
    "    lasso.fit(X_encoded.iloc[train], y.iloc[train])\n",
    "    \n",
    "    # Predict and score\n",
    "    Y_predict = lasso.predict(X_encoded.iloc[test])\n",
    "    R2.append(lasso.score(X_encoded.iloc[test], y.iloc[test]))\n",
    "    MAE.append(median_absolute_error(y.iloc[test], Y_predict))\n",
    "\n",
    "# Output and store scores after CV\n",
    "lasso_testing_set_score = np.mean(R2)\n",
    "lasso_median_abs_error = np.mean(MAE)\n",
    "print 'The R^2 score on our testing data is: ' + str(round(lasso_testing_set_score,3))\n",
    "print 'The median absolute error on our testing data is: ' + str(round(lasso_median_abs_error,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n",
    "Now we examine ensemble methods and see how they perform. We don't need to cross validate for our ensemble methods (only need to cross validate on hyper-parameter tuning) as the out of bag (OOB) error estimate gives an unbiased estimate of the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_encoded, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest  Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit random forest\n",
    "rf = RandomForestRegressor(oob_score = True)\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "# Predict and score RF\n",
    "Y_predict = rf.predict(X_test)\n",
    "rf_testing_set_score = rf.score(X_test, Y_test)\n",
    "rf_median_abs_error = median_absolute_error(Y_test, Y_predict)\n",
    "print 'The R^2 score on our testing data is: ' + str(round(rf_testing_set_score,3))\n",
    "print 'The median absolute error on our testing data is: ' + str(round(rf_median_abs_error,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuned RF Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters to run grid search on\n",
    "tuned_parameters = {\n",
    "    \"n_estimators\": [10, 30, 50],\n",
    "    \"n_jobs\": [-1],\n",
    "    \"oob_score\": [True],\n",
    "    \"max_features\": [None, 'log2', 'auto']\n",
    "}\n",
    "\n",
    "# Run 3-fold CV grid search to tune hyperparameters\n",
    "rf_tuned = GridSearchCV(RandomForestRegressor(), cv = 3, param_grid = tuned_parameters)\n",
    "\n",
    "# Score model\n",
    "preds = rf_tuned.fit(X_train, Y_train)\n",
    "best = rf_tuned.best_estimator_ \n",
    "Y_predict = rf_tuned.predict(X_test)\n",
    "rft_testing_set_score = rf_tuned.score(X_test, Y_test)\n",
    "rft_median_abs_error = median_absolute_error(Y_test, Y_predict)\n",
    "\n",
    "# Output score and model info\n",
    "print 'The best paramaters are:', rf_tuned.best_params_\n",
    "print 'The R^2 score on our testing data is: ' + str(round(rft_testing_set_score,3))\n",
    "print 'The median absolute error on our testing data is: ' + str(round(rft_median_abs_error,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost Regressor (with Random Forest as Base Estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit AdaBoost forest\n",
    "ada = AdaBoostRegressor(base_estimator = rf)\n",
    "ada.fit(X_train, Y_train)\n",
    "\n",
    "# Predict and score adaboost\n",
    "Y_predict = ada.predict(X_test)\n",
    "ada_testing_set_score = ada.score(X_test, Y_test)\n",
    "ada_median_abs_error = median_absolute_error(Y_test, Y_predict)\n",
    "\n",
    "print 'The R^2 score on our testing data is: ' + str(round(ada_testing_set_score,3))\n",
    "print 'The median absolute error on our testing data is: ' + str(round(ada_median_abs_error,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, when using AdaBoost on a random forest model, one is boosting using individual tree branches. However, to maximize the amount of information that we received from each boosting iteration, we decided that it might be better suited to boost on mutliple branches, possibly even an entire forest. By doing so, we will be able to better adjust our model to account for the misclassification and adapt our Random Forest model accoridngly. Keeping this in mind, we decided to run AdaBoost on our Tuned Random Forest. This means that instead of boosting using individual tree branches, we are boosting using an entire forest. Initially one of our main concerns was that such a method would prove to be too computationally expensive as far as run-time goes to be implemented. However, after discussing the idea with CS109a staff, most noteably Professor Protopapas, we were encouraged to explore whether this method would prove fruitful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuned AdaBoost Regressor (with Decision Tree as Base Estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters to run grid search on\n",
    "tuned_parameters = {\n",
    "    \"n_estimators\": [10, 25, 50, 100],\n",
    "    \"learning_rate\": [0.01, 0.1, 1],\n",
    "    \"loss\" : ['linear', 'square']\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(AdaBoostRegressor(), cv=3, param_grid=tuned_parameters)\n",
    "preds = clf.fit(X_train, Y_train)\n",
    "best = clf.best_estimator_ \n",
    "Y_predict = clf.predict(X_test)\n",
    "clf_testing_set_score = clf.score(X_test, Y_test)\n",
    "clf_median_abs_error = median_absolute_error(Y_test, Y_predict)\n",
    "print 'The R^2 score on our testing data is: ' + str(round(clf_testing_set_score,3))\n",
    "print 'The median absolute error on our testing data is: ' + str(round(clf_median_abs_error,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Model Results\n",
    "\n",
    "This method did indeed prove to much more computationally intensive than our other model, but yielded the best results out of all models. This is especially interesting considering that we did not tune AdaBoost, nor did we tune its base Random Forest estimator. The tuned Random Forest and tuned AdaBoost (with a Decision Tree regressor) were a close second to this model.\n",
    "\n",
    "It seems reasonable to believe that if we were to increase this number we would get increasingly more accurate results. However, we were not able to explore this possibility given the limited computational capacity at our disposal. Our AdaBoost with a Random Forest estimator had both the highest R^2 and lowest MAE, which shows that our method did prove effective in increasing accuracy. In this cause our median absolute error measures how far off on a log dollar basis our predicted prices from our model are from the actual test value. Therefore a smaller median absolute error, especially compared to the other method proves that this method is effective in increasing model accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $R^{2}$ Chart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Graph x and y axis values\n",
    "labels = np.array(['OLS','Ridge Tuned','Lasso Tuned', 'RF', 'Tuned (RF)', 'AB (RF)', 'AB (DT)'])\n",
    "error_val = np.array([lin_testing_set_score, ridge_testing_set_score, lasso_testing_set_score, \n",
    "                      rf_testing_set_score, rft_testing_set_score, ada_testing_set_score, \n",
    "                      clf_testing_set_score])\n",
    "\n",
    "# Arrange bars\n",
    "pos = np.arange(error_val.shape[0])\n",
    "srt = np.argsort(error_val)\n",
    "\n",
    "# Plots R^2 bars across functions\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(pos, error_val[srt], align = 'center', color=BNB_RED)\n",
    "plt.xticks(pos, labels[srt])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('$R^2$ Value')\n",
    "plt.title('$R^2$ Model Comparison')\n",
    "plt.ylim(0.5,0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Median Absolute Error Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Graph x and y axis values\n",
    "labels = np.array(['OLS','Ridge Tuned','Lasso Tuned', 'RF', 'Tuned (RF)', 'AB (RF)', 'AB (DT)'])\n",
    "error_val = np.array([lin_median_abs_error, ridge_median_abs_error, lasso_median_abs_error, \n",
    "                      rf_median_abs_error, rft_median_abs_error, ada_median_abs_error, \n",
    "                      clf_median_abs_error])\n",
    "\n",
    "# Arrange bars\n",
    "pos = np.arange(error_val.shape[0])\n",
    "srt = np.argsort(error_val)\n",
    "\n",
    "# Plots Mean Absolute Variance bars across functions\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(pos, error_val[srt], align = 'center', color=BNB_BLUE)\n",
    "plt.xticks(pos, labels[srt])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Median Absolute Error in Log($)')\n",
    "plt.title('Median Absolute Error Model Comparison')\n",
    "plt.ylim(0.1,0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the both the $R^2$ and the Median Absolute Error Chart, we can observe a trend such that model that have a high $R^2$ tend to have a low median absolute error values. This shouldn't be a surprise and should be expected because a high $R^2$ represents a more accurate model, as does a low median absolute error value. So we viewing both the $R^2$ and median absolute error graphs in tandem, it becomes quickly evident that our top three models are AdaBoost Tuned RF, AdaBoost Random Forest, and a regular random forest. From a pure accuracy metric, our AdaBoost with Random Forest as a base estimator yielded the most accurate results.\n",
    "\n",
    "**NOTE:** Due to the presence of extreme outliers and skewness in the data set we chose to evaluate our model's accuracy on a median absolute error basis rather than a mean absolute error basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictor_importance = rf_tuned.best_estimator_.feature_importances_\n",
    "# calculates predictor importance on a scale from 0 to 100\n",
    "predictor_importance = 100.0 * (predictor_importance / predictor_importance.max())\n",
    "\n",
    "# creates an index based on the sorted predictor importance\n",
    "index = np.argsort(predictor_importance)\n",
    "pos = np.arange(index.shape[0]) + 2.5\n",
    "pvals = predictor_importance[index]\n",
    "pcols = X_train.columns[index]\n",
    "\n",
    "# plots the figure\n",
    "plt.figure(figsize = (10,20))\n",
    "plt.barh(pos, pvals, align = 'center', color =BNB_RED)\n",
    "plt.yticks(pos, pcols)\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Predictor Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our variable importance feature selection is taken from our Tuned Random Forest model. We calculated predictor importance as a measure of how that predictor compares to the most important predictor and use this ratio / percentage to quantify the importance of each predictor. We then sort and plot our predictors on a horizontal bar chart, highlighting the most important featuers at the top and the less important features at the bottom of the graph.\n",
    "\n",
    "** Most Important Variables:**\n",
    "- ***room_type_0:*** this feature indicates that the listing room type is the entire home / apartment. This is appealing to guests that prefer privacy and are willing to pay a premium for this feature. It makes sense that this would be one of the most important variables because two listings can have the same features and asthetics, but differ on price mainly due to the fact that an entire home / apartment features one party as opposed to other room types that require the guest to share the space with others.\n",
    "\n",
    "\n",
    "- ***longitude & latitude:*** this captures the location of the listings. Certain neighborhoods and locations, due to their proximity to key attractions will have naturally more expensive properties than others.\n",
    "\n",
    "\n",
    "- ***dist_to_subway:*** as anticipated proximity to subway and other transit hubs plays a big impact in pricing. The ease of transportation associated with living closer to a subway station is attached with a price permium.\n",
    "\n",
    "\n",
    "- ***polarities:*** our polarity findings show that reviews do go a long way in affecting listing price. Users read reviews and form opinions on the listings, as measured by polarity, which heavily factors into whether they will decide to rent the listing.\n",
    "\n",
    "\n",
    "- ***availability_365:*** this feature more broadly represents availability as a whole (given that 30, 60, 90, and 365 day availability are all highly correlated). This is potentially a feature that captures the supply aspect of the listing. If there are a limited number of days for which the listing is available this supply factor may affect the price at which one decides the list their space.\n",
    "\n",
    "\n",
    "- ***number of reviews & review_score_ratings:*** reviews provide valuable insight as to how past guests have assessed the space that a prospective guest is staying in. The more reviews there are, the more information there is for a prospective guest to make an informed decision. Also review scores also go a long way into shaping a prospective guests opinion of the plan. Naturally listings with higher review scores will tend to have higher listing prices than those with lower review scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
